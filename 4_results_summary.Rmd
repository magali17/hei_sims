---
title: "ACT-TRAP Simulations Results Summary"
author: "Magali Blanco"
date: ' `r Sys.time()` '
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
    number_sections: true
    toc_float: true
    collapsed: false
    smooth_scroll: false
editor_options: 
  chunk_output_type: console
params:
  args: myarg
---

# . 

```{r, setup, include=F}
# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
  res <- suppressWarnings(
    lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
           detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

knitr::opts_chunk$set(echo = F, 
                      cache=F, cache.comments = F, 
                      message = F, warning = F, 
                      tidy.opts=list(width.cutoff=60), tidy=TRUE,
                      fig.height = 8, fig.width = 8
                      )  

pacman::p_load(#kable, 
               kableExtra, 
               broom,  #tidy()
               ggpubr,
               tidyverse,
               ggmap, sf, ggspatial, #mapping...adding scales, N arrows
               units, #convert between e.g., m to km
               VCA, #anovaVCA()
               parallel #mclapply()
               )    
 
# ggplot settings
theme_set(theme_bw())
theme_update(legend.position = "bottom")


set.seed(1)

image_path <- file.path("..", "Manuscript", "Images")

```



Functions

```{r}
# fn labels variables

label_pollutant <- function(dt) {
  
  dt <- dt %>%
    mutate(
     variable = case_when(
       variable == "co2_umol_mol" ~ "CO2 (ppm)", 
       variable == "ma200_ir_bc1" ~ "BC (ng/m3)", 
       variable == "no2" ~ "NO2 (ppb)", 
       variable == "pm2.5_ug_m3" ~ "PM2.5 (ug/m3)",
       variable == "pnc_noscreen" ~ "PNC (pt/cm3)",
       TRUE ~ variable
       ),
     variable = factor(variable, levels = c("PNC (pt/cm3)", "BC (ng/m3)", "NO2 (ppb)", "PM2.5 (ug/m3)", "CO2 (ppm)"))
     )
  
  return(dt)
      
  }
```


```{r}

# facet_wrap_equal() function acts like facet_wrap() in ggplot but it sets the axes ranges (min/max) of each facet to same scale so that the 1-1 line is always down the middle :D !!

# code source: https://fishandwhistle.net/post/2018/modifying-facet-scales-in-ggplot2/ 

FacetEqualWrap <- ggproto(
  "FacetEqualWrap", FacetWrap,
  
  train_scales = function(self, x_scales, y_scales, layout, data, params) {
    
    # doesn't make sense if there is not an x *and* y scale
    if (is.null(x_scales) || is.null(x_scales)) {
      stop("X and Y scales required for facet_equal_wrap")
    }
    
    # regular training of scales
    ggproto_parent(FacetWrap, self)$train_scales(x_scales, y_scales, layout, data, params)
    
    # switched training of scales (x and y and y on x)
    for (layer_data in data) {
      match_id <- match(layer_data$PANEL, layout$PANEL)
      
      x_vars <- intersect(x_scales[[1]]$aesthetics, names(layer_data))
      y_vars <- intersect(y_scales[[1]]$aesthetics, names(layer_data))
      
      SCALE_X <- layout$SCALE_X[match_id]
      ggplot2:::scale_apply(layer_data, y_vars, "train", SCALE_X, x_scales)
      
      SCALE_Y <- layout$SCALE_Y[match_id]
      ggplot2:::scale_apply(layer_data, x_vars, "train", SCALE_Y, y_scales)
    }
    
  }
)

facet_wrap_equal <- function(...) {
  # take advantage of the sanitizing that happens in facet_wrap
  facet_super <- facet_wrap(...)
  
  ggproto(NULL, FacetEqualWrap,
          shrink = facet_super$shrink,
          params = facet_super$params
  )
}


#same as above but for facet_grid()
FacetEqualGrid <- ggproto(
  "FacetEqualGrid", FacetGrid,
  
  train_scales = function(self, x_scales, y_scales, layout, data, params) {
    
    # doesn't make sense if there is not an x *and* y scale
    if (is.null(x_scales) || is.null(x_scales)) {
      stop("X and Y scales required for facet_equal_wrap")
    }
    
    # regular training of scales
    ggproto_parent(FacetGrid, self)$train_scales(x_scales, y_scales, layout, data, params)
    
    # switched training of scales (x and y and y on x)
    for (layer_data in data) {
      match_id <- match(layer_data$PANEL, layout$PANEL)
      
      x_vars <- intersect(x_scales[[1]]$aesthetics, names(layer_data))
      y_vars <- intersect(y_scales[[1]]$aesthetics, names(layer_data))
      
      SCALE_X <- layout$SCALE_X[match_id]
      ggplot2:::scale_apply(layer_data, y_vars, "train", SCALE_X, x_scales)
      
      SCALE_Y <- layout$SCALE_Y[match_id]
      ggplot2:::scale_apply(layer_data, x_vars, "train", SCALE_Y, y_scales)
    }
    
  }
)

facet_grid_equal <- function(...) {
  # take advantage of the sanitizing that happens in facet_wrap
  facet_super <- facet_grid(...)
  
  ggproto(NULL, FacetEqualGrid,
          shrink = facet_super$shrink,
          params = facet_super$params
  )
}


```

```{r}
#add appropriate labels for the fewer stops design
# ONLY RELEVANT FOR THE TOTAL STOPS DESIGN: clarify the number of simulation visits, sites and total stops 

relabel_fewer_stops <- function(dt) {
  dt %>%  
  #group_by(variable, design, version, campaign, out_of_sample) %>%
  mutate(
    #sites = as.character(length(unique(location))),
    
    # for total stops design, use the design #s (not necessarily the actual simulation #s - there may be minor discrepancies)
    sites = ifelse(grepl("stops", design), str_extract(version, " [0-9]{1,3}"), NA),
    sites = as.numeric(gsub(" ", "", sites)),
    visits = ifelse(grepl("stops", design), as.numeric(str_extract(version, "^[0-9]{1,2}")), visits),
    # NOTE: this is only accurate for the total stops design. e.g., sites have a different number of total visits (21-26)
    total_stops = visits * sites,
    # the actual max for this fewer total stops version
    total_stops = ifelse(total_stops == 26*278, 7080, total_stops),
    version = ifelse(grepl("stops", design), as.character(total_stops), version)
    ) #%>% ungroup()
  
}

```


# Upload Data 


```{r}
# mapping variables
project_crs <- 4326  #lat/long
m_crs <- 32148

# 2-min winsorized medians used in simulations
stops <- readRDS(file.path("Output", "stops_used.rda")) %>%  
  #gather(variable, value, co2_umol_mol:pnc_noscreen)
  pivot_longer(c("ma200_ir_bc1", "co2_umol_mol", "no2", "pm2.5_ug_m3", "pnc_noscreen") ,
               #co2_umol_mol:pnc_noscreen, 
               names_to ="variable", values_to = "value") 

# uk predictions
predictions0 <- readRDS(file.path("Output", "UK Predictions", "all_predictions.rda" #"predictions.rda"
                                  )) %>% 
  #gather("reference", "estimate", contains("estimate")) %>%
  pivot_longer(contains("estimate"), names_to = "reference", values_to =  "estimate") %>%
  # some campaigns don't have "estimtes" for test set locations
  drop_na(estimate) %>%
  
  # ONLY RELEVANT FOR THE TOTAL STOPS DESIGN: clarify the number of simulation visits, sites and total stops 
  relabel_fewer_stops() %>%
  
  select(
    spatial_temporal, design, version, visits, sites, total_stops, campaign, variable, out_of_sample, location, prediction, reference, estimate 
  )




# simulation details
sims0 <- readRDS(file.path("Output", "annual_training_set.rda")) %>%  
  distinct(location, route, visits, campaign, design, version, spatial_temporal) %>%
  # ONLY RELEVANT FOR THE TOTAL STOPS DESIGN: clarify the number of simulation visits, sites and total stops 
  relabel_fewer_stops()

# #location lat/long 
# loc_lat_long <- readRDS(file.path("Output", "location_lat_long.rda")) %>%
#   st_as_sf(coords = c('longitude', 'latitude'), crs=project_crs, remove = F) %>%
#   st_transform(m_crs)  

model_eval0 <- readRDS(file.path("Output", "model_eval.rda"))%>%
  
  # ONLY RELEVANT FOR THE TOTAL STOPS DESIGN: clarify the number of simulation visits, sites and total stops don't use relabel_fewer_stops() b/c this df doesn't have visits column
  mutate(
    sites = ifelse(grepl("stops", design), str_extract(version, " [0-9]{1,3}"), NA),
    sites = as.numeric(gsub(" ", "", sites)),
    visits = ifelse(grepl("stops", design), as.numeric(str_extract(version, "^[0-9]{1,2}")), NA),
    total_stops = visits * sites,
    # the actual max for this fewer total stops version
    total_stops = ifelse(total_stops == 26*278, 7080, total_stops),
    
    version = ifelse(grepl("stops", design), as.character(total_stops), version)
    )

test_locations <- predictions0 %>%
  filter(out_of_sample == "Test") %>% 
  distinct(location) %>% pull()

#location lat/long 
loc_lat_long <- readRDS(file.path("Output", "location_lat_long.rda")) %>%
  st_as_sf(coords = c('longitude', 'latitude'), crs=project_crs, remove = F) %>%
  st_transform(m_crs) %>%
  mutate(route = factor(route),
         set = ifelse(location %in% test_locations, "Test Set", "Simulation Set")
         )
          
# monitoring area shp 
monitoring_region <- readRDS(file.path("..", "..", "1. Our Campaign", "Our Campaign R", "Data", "Output", "GIS", "monitoring_land_shp.rda")) %>%
  st_transform(m_crs)

# grid predictions
grid_predictions0 <- readRDS(file.path("Output", "UK Predictions", "grid_predictions.rda"))  %>%
  label_pollutant() %>%
  # calculate summary statistics for each location across campaigns
  group_by(location_id, longitude, latitude, spatial_temporal, design, version, variable) %>%
  summarize(
    prediction_median = median(prediction),
    prediction_iqr = IQR(prediction)
  ) %>%
  # calculate median prediction difference relatie to full campaign prediction
  group_by(variable, location_id) %>%
  mutate(median_prediction_diff = prediction_median -  prediction_median[grepl("full", design)]) %>%
  ungroup() %>%
  
  # ONLY RELEVANT FOR THE TOTAL STOPS DESIGN: clarify the number of simulation visits, sites and total stops don't use relabel_fewer_stops() b/c this df doesn't have visits column
  mutate(
    sites = ifelse(grepl("stops", design), str_extract(version, " [0-9]{1,3}"), NA),
    sites = as.numeric(gsub(" ", "", sites)),
    visits = ifelse(grepl("stops", design), as.numeric(str_extract(version, "^[0-9]{1,2}")), NA),
    total_stops = visits * sites,
    # the actual max for this fewer total stops version
    total_stops = ifelse(total_stops == 26*278, 7080, total_stops),
    
    version = ifelse(grepl("stops", design), as.character(total_stops), version)
    ) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs=project_crs,  remove = F)

```

```{r}
# additional tables 
## proportion of sites used in each distance buffer
sites_used_geo_dist <- readRDS(file.path("Output", "SI", "sites_used_geo_dist.rda"))
sites_used_pca_dist <- readRDS(file.path("Output", "SI", "sites_used_pca_dist.rda"))

# UK-PLS modeling parameters
modeling_parameters <- readRDS(file.path("Output", "full_model_parameters.rda"))

```



```{r}
# calculates existing levels for numeric version options
sort_num_lvs <- function(dt = model_eval0, design_name) {
  lvls <- dt %>%
    filter(design %in% design_name) %>% 
    distinct(version) %>% pull() %>% 
    #make sure order is correct
    as.character() %>% as.numeric() %>% sort() %>% 
    as.character()
  
  return(lvls)
}

```

### --> update using new cluster/pca code

```{r}
#unique(model_eval0$spatial_temporal)
st_levels <- c("gold standard", "spatial temporal", "spatial", "temporal")

design_levels <- c("full", 
                   "fewer total stops",
                   "balanced seasons", "fewer days", "fewer hours", 
                   str_subset( unique(model_eval0$design), "distance")
                   )

version_levels <- c("all training data",
                    sort_num_lvs(design_name =  c("fewer total stops", "balanced seasons", 
                                                  str_subset( unique(model_eval0$design), "distance"))),
                                                  #"geographic distance (m)", "PCA covariate distance")),
                    "business", "business & rush", "rush",
                    unique(model_eval0$version[model_eval0$design == "fewer days"])
                    ) %>%
  unique()

oos_lvls <- c("Test", str_subset (unique(model_eval0$out_of_sample), "CV"), "Buffered LOO")

```

```{r}
#fn orders factor levels
add_lvls <- function(dt, spatial_temporal. = st_levels, design_levels. = design_levels, version_levels. = version_levels, oos_lvls. = oos_lvls) {
  
  dt %>%
    mutate(
      spatial_temporal = factor(spatial_temporal, levels = spatial_temporal.),
      design = factor(design, levels = design_levels.),
      version = factor(version, levels = version_levels.),
      out_of_sample = factor(out_of_sample, levels = oos_lvls.)
    )
}

```


```{r}
model_eval <- #add_lvls(model_eval0) %>%
  model_eval0 %>%
  mutate(
    version = factor(version, levels = version_levels),
    design = factor(design, levels = design_levels),
    out_of_sample = factor(out_of_sample, levels = oos_lvls)
  ) %>% 
  label_pollutant()
  
geo_dist_ref <- sort_num_lvs(design_name =  str_subset(unique(model_eval0$design), "geographic"))[1]
cov_dist_ref <- sort_num_lvs(design_name =  str_subset(unique(model_eval0$design), "covariate"))[1]

normalized_model_eval <- model_eval %>%
  pivot_longer( c("RMSE", contains("_R2")),names_to =  "model_eval", values_to = "value") %>%
  #calculate normalized RMSE, using ___ value as denominator  
  group_by(variable, model_eval, out_of_sample, reference) %>%
  mutate(
    normalized_value = ifelse( out_of_sample != "Buffered LOO", value/value[design == "full"] , #normalized_value
                               ifelse(grepl("geographic", design), value/value[version== geo_dist_ref],
                                      ifelse(grepl("covariate", design), value/value[version== cov_dist_ref], NA)
                                      ))
    )

  
sims <- sims0 %>%
  mutate(
    spatial_temporal = factor(spatial_temporal, levels = st_levels),
      version = factor(version, levels = version_levels),
      design = factor(design, levels = design_levels),
      )


## NOTE! The "visits", "sites", and "total_stops" columns are correct for cross-validated predictions in that they describes the simulation design used to generate the predictions. For fully ou-of-sample test sites, HOWEVER, these columns are meanigless. For example:   
# predictions %>% group_by(design, version, out_of_sample) %>% summarize(unique_visits = paste(unique(visits), collapse = ", ")) %>% View()

### --> Note the total # of stops is slighlty lower for the fewer total stops 24+ visits b/c some sites don't have that many visits

predictions <- add_lvls(predictions0) %>%
  label_pollutant() 
  

#predictions %>% filter(out_of_sample == "CV", grepl("total stops", design), grepl("PNC", variable)) %>% View()


# grid predictions
grid_predictions <- grid_predictions0 %>%
  mutate(
    design = factor(str_to_title(design), levels = str_to_title(design_levels)),
    version = factor(str_to_title(version), levels = str_to_title(version_levels))
  ) 


```

```{r}
# common vars

var_names <- levels(predictions$variable)

# for plots later
design_list <- list(
  #don't include extrapollation
  str_subset(unique(model_eval$design), "distance|fewer total stops", negate = T),
  # full & extrapollation
  str_subset(unique(model_eval$design), "full|distance")  
   
)

```


# Stops used

### --> add more?

```{r}
# of stops per site
stops %>%
  filter(variable==first(variable)) %>%
  group_by(location) %>%
  summarize(no_visits = n()) %>%
  group_by(no_visits) %>%
  summarize(sites = n()) %>%
  mutate(visit_group = ifelse(no_visits < 26, "less than 26",
                              ifelse(no_visits > 26, "more than 26", "26")
                              )) %>%
  group_by(visit_group) %>%
  mutate(sites_in_visit_group = sum(sites)) %>%
  kable(caption = "number of visits per site") %>% 
  kable_styling()


```


# Simulation Descriptions






range of times used for calculating annual averages; number of stops; check that temporal balance remains after dropping initial stops

* avg # of visits is lower because only visits with all pollutant measures were used. Number varied for simulations w/ fewer random site visits.

```{r}
stops_visits <- predictions %>%
  filter(grepl("stops", design)) %>%
  arrange(sites, visits)
  # mutate(
  #   visits = as.numeric(str_extract(version, "^[0-9]{1,2}")),
  #   sites = as.numeric(str_extract(version, " [0-9]{1,3}")),
  #   sites = as.numeric(gsub(" ", "", sites)),
  #   total_stops = sites*visits
  #   )  


sims %>%
  group_by(spatial_temporal, design, version) %>%
  summarize(
    no_campaigns = max(campaign),
    no_sites = length(unique(location)),
    avg_visits_per_site = round(mean(visits))
    ) %>% 
  mutate(spatial_temporal = factor(spatial_temporal, levels = c("gold standard", "spatial temporal", "spatial", "temporal"))) %>%
  arrange(spatial_temporal) %>% 
  
  group_by(spatial_temporal, design) %>%
  summarize(
    version = paste(unique(version), collapse = ", "),
    no_sites = paste(unique(no_sites), collapse = ", "),
    avg_visits_per_site = paste(unique(avg_visits_per_site), collapse = ", "),
    #no_campaigns = unique(no_campaigns)
  ) %>%  
  mutate(
    no_sites = ifelse(grepl("stops", design), "varies", no_sites),
    avg_visits_per_site = ifelse(grepl("visits|stops", design), "varies", avg_visits_per_site),
    
    version = ifelse(design == "fewer total stops", paste0(paste(range(stops_visits$total_stops), collapse = "-") ," total stops, based on a varying number of sites (", paste(unique(stops_visits$sites), collapse = ", "), ") and visits ( ",  paste(unique(stops_visits$visits), collapse = ", "), ")"),  version)
  ) %>%
  
  kable(caption = "Simulation Descriptions. The distance extrapollation analyses use the full campaign to conduct buffered LOO CV.", 
        col.names = c("Design Type", "Design", "Versions", "Unique Sites", "Visits per Site"#, "No. Campaign Simulations"
                      )
        ) %>%
  kable_styling() %>%
  add_footnote(c("average visits per site for Designs: full",
                 "the fewer site design versions can have different sites across simulations"
                 ))
  
```


* number of sites & monitor density


```{r, eval=F}
### --> redo w/ "fewer total stops" instead? 

monitor_density_crosswalk <- model_eval %>%
  filter(design %in% c("fewer sites", "full")) %>%
  mutate(
    version = ifelse(grepl("full", design), first(training_sites[design=="full"]), as.numeric(as.character(version) ))
  ) %>%
  distinct(version, training_monitors_per_100km2 = round(training_monitors_per_100km2)) %>%
  arrange(version) 

monitor_density_crosswalk %>%
  kable(caption = "Monitoring density when using fewer sites", 
        col.names = c("Total No. Sites", "Sites per 100 km2")
          ) %>%
  kable_styling()


```





# Map of Stops


```{r}
# create background map for grid/mapping
#bbox <- st_bbox(act_shp)

#make box little bigger than monitoring area
bbox <- st_bbox(st_transform(st_buffer(loc_lat_long, 10000), project_crs) )

names(bbox) <- c("left", "bottom", "right", "top")

# background map
map0 <- suppressMessages(get_stamenmap(bbox = bbox, zoom = 11, maptype = "terrain" )) %>%
  # Make basic map image from the tiles
  ggmap(ggmap = ., darken = c(0.5, "white")) + theme_void()

## usage example: 
# map0 + geom_point()

```

## main 

* test set sites =31 (black dots)      
* sites available for simulations = 278   

```{r, fig.height=10}

map0 + 
  #monitoring area
  geom_sf(data = st_transform(monitoring_region, project_crs), inherit.aes = F, lwd = 0.1, alpha = 0.1,
          #aes(fill = "Monitoring\nArea")
          ) + 
  
  #monitoring stops
  geom_sf(data = st_transform(loc_lat_long, project_crs), aes(col = "Available\nfor Training"), inherit.aes = F, size=2) + 
  
  # # test set - black dots
  geom_sf(data = filter(st_transform(loc_lat_long, project_crs), grepl("Test", set)), inherit.aes = F, size=2, aes(col = "Test Set")) +
  theme_bw() +
  
  theme(legend.justification=c(1,1), legend.position=c(1,1),
        legend.background =  element_blank()
        ) +
  scale_color_manual(values = c("darkorchid1", "black")) +
  
  # add scale & N arrow to top rught
    annotation_scale(location = "tl") +
    annotation_scale(location = "tl", unit_category ="imperial", pad_y = unit(0.55, "cm")) +
  annotation_north_arrow(location = "tl", pad_y = unit(0.5, "in"), style = north_arrow_fancy_orienteering) +
  
  #add attribution/reference to bottom left
  geom_text(aes(x=-Inf, y=-Inf, hjust=-0.01, vjust=-0.5, label= "Map tiles by Stamen Design, under CC BY 3.0. \nData by OpenStreetMap, under ODbL."), size=3) +
    
  labs(x = "Longitude", y = "Latitude", col = "Site", fill="") 

ggsave(file.path(image_path, "SI", "training_test_sites_map.png"), width = 5, height = 7)

```



# 2-min medians  

```{r}
stops %>% 
  ggplot(aes(x=value)) + 
  facet_wrap(~variable, scales = "free") + 
  geom_histogram() 

```

```{r}

stops %>%
  label_pollutant() %>%
  group_by(Pollutant = variable) %>%
  summarize(
    #N = n(), # N = 7327
    #Min = min(value),
    Q05 = quantile(value, 0.05),
    Q25 = quantile(value, 0.25),
    Median = median(value),
    Mean = mean(value),
    Q75 = quantile(value, 0.75),
    Q95 = quantile(value, 0.95),
    #Max = max(value),
    #"Max/Min" = Max/Min
    "Q95/Q05" = round(Q95/Q05, 1)
  ) %>% 
  mutate_at(vars(Q05:Q95), ~ ifelse(grepl("PM2.5", Pollutant), round(., 1), round(.))) %>%
  mutate_at(vars(Q05:Q95), ~ format(., big.mark = ",")) %>%
  
  kable(caption = "Distribution of median stop concentrations. N = 7,327 total stops with measurements for all pollutants") %>%
  kable_styling()
  
  


```



## ANOVA

variability 

```{r}


#x = group_split(stops, variable)[[1]]

anova1 <- mclapply(group_split(stops, variable), mc.cores = 4,  function(x) {
  
  temp <- anovaVCA(value ~ location + season + day + hour, Data = as.data.frame(x))$aov.tab  %>% 
    tidy() %>%
    select(factor = .rownames, percent = X.Total) %>% 
    mutate(variable = first(x$variable))
  }) %>%
  bind_rows() %>% 
  filter(factor != "total") %>%
  mutate(
    #factor = ifelse(grepl("tow2", factor), "day", factor),
    factor = ifelse(factor == "location", "site", factor),
    factor = factor(factor, levels = c("error", "site", "season", "day", "hour")),
    kind = ifelse(grepl("site", factor), "spatial",
                  ifelse(grepl("day|hour|season", factor), "temporal", "residual")
                  ),
    kind = factor(kind, levels = c("residual", "temporal", "spatial"))
  ) %>%
  label_pollutant()


```

```{r}

print("Percent of pollutant variability explained by spatial and temporal factors. ANOVA models are for stop-level concentration values (2-min stop), such that: value ~ location (df=277) + season (df=3) + tow2 (df=1) + hour (df~19). The error term is the residual variability for any given site after adjusting for season, day of the week (weekend vs weekday) and hour of the day.")

anova1 %>%
  ggplot(aes(x=kind, y=percent, #col=factor, 
             fill=factor)) + 
  geom_bar(stat = "identity") + 
  facet_grid(~variable, switch = "both") + 
  labs(
    y = "Percent (%)",
    x = "Variability",
    fill = ""
  )

ggsave(file.path(image_path, "SI", paste0("stop_level_anovas.png")), width = 9, height = 5)

```


```{r}
anova1 %>%
  group_by(variable, kind) %>%
  summarize(percent = round(sum(percent))) %>%
   #spread(variable, percent) %>% 
  pivot_wider(names_from = variable, values_from = percent) %>%
  kable(caption = "Percent of variability explained by pollutant models") %>%
  kable_styling()
  

```



# Annual averages 

## estimates 

distribution of annual averages from full campaign (n=278 sites)

```{r}
# table
predictions %>%
  filter(grepl("full", design),
         grepl("campaign", reference)
         ) %>% 
  pivot_longer(c(prediction, estimate), names_to = "estimate_type", values_to = "value") %>%
  group_by(Pollutant = variable, Type = estimate_type) %>%
  summarize(
    #N = n(), # N = 278
    Q05 = quantile(value, 0.05),
    Q25 = quantile(value, 0.25),
    Median = median(value),
    Mean = mean(value),
    Q75 = quantile(value, 0.75),
    Q95 = quantile(value, 0.95),
    "Q95/Q05" = round(Q95/Q05, 1)
  ) %>%
  mutate_at(vars(Q05:Q95), ~ ifelse(grepl("PM2.5", Pollutant), round(., 1), round(.))) %>%
  mutate_at(vars(Q05:Q95), ~ format(., big.mark = ",")) %>%
  
  kable(caption = "Distribution of annual average site estimates and model predictions from the full campaign. N = 278 sites for all pollutants.") %>%
  kable_styling()
  
  
```



```{r}
# plot 
predictions %>%
  filter(grepl("full", design),
         grepl("campaign", reference)
         ) %>%
  ggplot(aes(x=estimate)) +
  facet_wrap(~variable, scales="free") +
  geom_histogram() +
  labs(title = "Distribution of annual average site estimates")

```

annual average estimates for simulations 

```{r, fig.height=10, fig.width=10}

predictions %>%
  filter(grepl("campaign", reference),
          #don't include site-visits together
         #!grepl("fewer total stops", design)
         !grepl("stops", design)
         ) %>%

  group_by(design, version) %>%
  mutate(avg_visits = round(mean(visits))) %>% 
  
  ggplot(aes(y=version, col=avg_visits, x=estimate)) + 
  facet_grid(design~variable, scales="free", switch = "y", space = "free_y") + 
  geom_boxplot() +
  # reverse color
  scale_color_continuous(low="#56B1F7", high="#132B43") +
  
  labs(col = "Avg Visits Per Site")

```



 

## predictions 

```{r, fig.height=10}

for(i in design_list) {
#i = design_list[1] %>% unlist()

  p <- predictions %>%
  filter(design %in% unlist(i)) %>%  
  
  ggplot(aes(x = prediction,
             y=version,
             col = out_of_sample
             ),
         ) + 
  facet_grid(design~variable, scales = "free", switch = "y" ) + 
  geom_boxplot() +
  labs(x = "Prediction",
       col= "Out-of-Sample Set"
        )
  
  print(p)
  
}

```

### correlations 

```{r}
temp0 <- predictions %>%
  filter(reference == "gs_estimate",
         out_of_sample == "CV",
         # don't include buffered or sites/visits designs
         !grepl("distance", design)
         ) %>% #View()
  select(location, campaign, design, version, spatial_temporal, variable, prediction, out_of_sample)#%>% 
  #spread(variable, prediction) 

temp_full <- temp0 %>%
  filter(design == "full") %>%
  rename(full_prediction = prediction) %>% 
  select(location, variable, full_prediction)



temp <- temp0 %>%
  filter(!design == "full") %>% 
  left_join(temp_full)  %>%
  group_by(design, version, variable, 
           campaign
           ) %>%
  summarize(
    n = n(),
    cor = cor(prediction, full_prediction)
    ) %>%
  mutate(
    design = factor(str_to_title(design), levels = c(str_to_title(levels(temp0$design)))),
    version = factor(str_to_title(version), levels = c(str_to_title(levels(temp0$version)))),
    
    #bin fewer total stops
      version = ifelse(grepl("total stops", design, ignore.case = T), 
                       as.character(cut(as.numeric(as.character(version)), breaks = seq(0,8000,1000), labels = c(seq(1000,7000,1000), 7080), dig.lab=5) ), 
                       as.character(version))
    
    )


```


```{r}
print("Correlations between cross-validated site predictions from the full campaign and other campaigns. Each boxplot represents 30 campaign correlations.")

temp %>%
  ggplot(aes(x=version, fill=variable,y=cor)) + 
  facet_grid(cols = vars(design), rows = vars(variable),  scales = "free_x", switch = "both", 
             #space = "free_x", 
             ) +
  #geom_point() +
  # "good" correlation
  geom_hline(yintercept = 0.9, linetype=2, alpha=0.6) +
  # refernce for how many stops temporal designs have
  geom_vline(xintercept = as.character(12*278), linetype=3, alpha=0.5) +
  geom_boxplot() +
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE)) +
  labs(fill = "Pollutant", x= "", y = "Correlation" )

ggsave(file.path(image_path, "SI",  "design_vs_full_campaign_cors.png"), width = 12, height = 10)

```




### Predition vs GS Estimates

 
```{r}
# calc difference between full campaign and restricted sampling design predictions
## looking at CV predictions

prediction_diff <- predictions %>%
  filter(
    grepl("gs_", reference),
    grepl("CV", out_of_sample, ignore.case = T),
    !grepl("distance", design) #fewer sites & visits|
  ) %>%
  group_by(location, variable) %>% #View()
  mutate( 
    prediction_diff = prediction - estimate[design == "full"],
    ) 

```

boxplots of error 


```{r}

prediction_diff %>%
  group_by(variable, design, version) %>%
  summarize(
    qmin = quantile(prediction_diff, 0.05),
    q25 = quantile(prediction_diff, 0.25),
    median = median(prediction_diff),
    q75 = quantile(prediction_diff, 0.75),
    qmax = quantile(prediction_diff, 0.95),
    ) %>%
  mutate(version = ifelse(version == "business & rush", "business \n& rush", as.character(version)),
         version = factor(str_to_title(version), levels = c(str_to_title(version_levels), "Business \n& Rush" )),
         design = factor(str_to_title(design), levels = c("Full", "Fewer Total Stops", "Balanced Seasons", "Fewer Days", "Fewer Hours")),) %>% 

  ggplot(aes(x=version, fill=variable)) +
  facet_grid(cols = vars(design), rows = vars(variable), scales = "free", switch = "y") +
  
  #geom_boxplot(aes(y=prediction_diff)) +
  geom_boxplot(aes(ymin = qmin, lower = q25, middle = median, upper = q75, ymax = qmax), stat = "identity", alpha=0.8) +
  
  geom_hline(yintercept = 0, linetype=2, alpha=0.5) +
  
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE)) +

  labs(fill = "Pollutant", x= "", y = "Prediction Error" )


ggsave(file.path(image_path, "SI",  "site_prediction_diff.png"), width = 10, height = 10)

```


 


```{r, fig.height=10, eval=F}
# ### prediction error, relative to GS estimate 


# OLD TEXT: * **each boxplot represents `r max(sims$campaign)` simulations**


for(i in design_list) {
#i = design_list[1] %>% unlist()

  p <- predictions %>%
  mutate(error = prediction-estimate) %>%
  filter(design %in% unlist(i)) %>%  
    
    #?only show mean bias?
    group_by(variable, design, version, out_of_sample) %>%
    summarize(
      error = mean(error)
      ) %>%
  
  ggplot(aes(x = error,
             y=version,
             col = out_of_sample
             ),
         ) + 
  facet_grid(design~variable, scales = "free", switch = "y" ) + 
  geom_boxplot() +
  geom_vline(xintercept = 0, linetype=2, alpha=0.5) +

  labs(x = "Mean Prediction Error",
       col = "Validation Set"
        )
  
  print(p)
  
}

```
 






```{r}
# Kriging has an assumption of normally distributed residuals. log transform if these are not normally distributed. 
# 
# ### --> ? *look at distribution of residuals at 280 sites after subtracting regression part 

```


scatterplots 

* estimates are from the gold standard (how RMSE, R2 is being evaluated)

```{r, fig.height=10, eval=F}

predictions %>%
  filter(grepl("gs_", reference),
         !grepl("full", design),
         #only have estimate-prediction pairs for these locations
         grepl("CV", out_of_sample),
         
         #don't include site-visits together
         #!grepl("fewer sites & visits", design)
         
         ) %>%
  ggplot(aes(x=estimate, y=prediction, 
               col=version
               )) +
  facet_wrap_equal(design~variable, scales="free", ncol=5) +
  theme(aspect.ratio=1,
        legend.position = "right"
        ) +
  geom_point(alpha=0.01) +
  geom_smooth(se=F, method = "lm") +
  #scale_colour_manual(values = rainbow(length(unique(predictions$version)))) +
  geom_abline(slope = 1, intercept = 0, linetype=2, alpha=0.5) +
  labs()

```





# Model Performances

* pollutants were all modeled on the log scale, but evaluated on the native scale 



```{r}
model_eval %>%
  filter(grepl("gs_", reference),
         #only look at full campaign
         grepl("full", design)
         ) %>%  
  select(out_of_sample, variable, RMSE, MSE_based_R2) %>%
  arrange(desc(out_of_sample), variable) %>%
  kable(caption = "Model performance of the full campaig. The 'GS' and 'simulated campaign' reference values are the same here.", digits = 2) %>%
  kable_styling()

```





## Scatter plots

### full campaign 

* high concentration sites tend to be underpredicted, especially PNCs

scatterplot of full campaign 

```{r, fig.height=10}
# compare GS estimest vs campaign predictions at test set (used below in RMSE, etc.) 
t0 <- model_eval %>%
  filter(grepl("gs_", reference),
         #only look at full campaign
         grepl("full", design)
         ) %>%  
  select(out_of_sample, variable, RMSE, MSE_based_R2) %>% 
  mutate(
    MSE_based_R2 = format(round(MSE_based_R2, 2), nsmall = 2),
    RMSE = #ifelse(grepl("PM2.5|NO2", variable), round(RMSE,1), round(RMSE) )
            ifelse(RMSE <10, round(RMSE, 1), round(RMSE))
            ) %>%
  
  
  group_by(variable) %>%
  summarize(label = paste0("CV: RMSE = ", RMSE[1], ", ","R2 = ", MSE_based_R2[1] , "\n",
                          "Test: RMSE = ", RMSE[2], ", ","R2 = ", MSE_based_R2[2]
                   )
            )


predictions %>%
  filter(grepl("gs_", reference),
         #only look at full campaign
         grepl("full", design)
         ) %>%
  
    ggplot(., aes(x=estimate, y=prediction, col=out_of_sample)) +
      facet_wrap_equal(~variable, scales="free") +
      theme(aspect.ratio=1) +
      geom_point(alpha=0.4) +
      geom_smooth(se=F, method = "lm") +
      
      #geom_text(data= t0, aes(x=-Inf, y=Inf, label=label), hjust=0, vjust=1.1, size=3, inherit.aes = F) +
  geom_text(data= t0, aes(x=Inf, y=-Inf, label=label), size=3, inherit.aes = F,
            hjust=1, vjust=-0.1 ) +
  
      geom_abline(slope = 1, intercept = 0, linetype=2, alpha=0.5) +
      geom_label(inherit.aes = F, aes(x=Inf, y=Inf),  hjust = 1, vjust=1, label="1:1", size=3) +
      
      geom_abline(slope = c(1.25, 0.75), intercept = 0, linetype=2, alpha=0.5) +

      # geom_label_npc(inherit.aes = F, aes(npcx=0.75, npcy=Inf, label = "+25%"), size=3) +
      # geom_label_npc(inherit.aes = F, aes(npcx=Inf, npcy=0.75, label = "-25%"), size=3) +
  
  labs(col = "Out-of-Sample") 


ggsave(file.path(image_path, "SI", "gs_scatterplot.png"), width = 9, height = 7)

```


 



```{r, eval=F}
# * trimmed vs untrimmed estimates

q0 <- 0.05

# comparison of trimmed (quantile: `r q0`) vs untrimmed annual average estimates at training locations

```


```{r, eval=F}
# stops used in simulations
stops0 <- readRDS(file.path("Output", "stops_used.rda"))  %>%
  pivot_longer(co2_umol_mol:pnc_noscreen, names_to = "variable",values_to =  "value") %>%
  label_pollutant()

stops <- stops0 %>%
  group_by(variable, location) %>%
  filter(
     value >= quantile(value, q0),
     value <= quantile(value, 1-q0),
    #value < max(value),
   ) %>% #filter(location == "MS0229", grepl("PNC", variable)) %>% summarise(mean(value))  
  summarize(estimate_trim = mean(value))


predictions %>%
  filter(grepl("full", design),
         grepl("campaign", reference)) %>% 
  select(location, variable, estimate) %>%
  right_join(stops) %>%
  drop_na() %>%
  
  ggplot(aes(x=estimate, y=estimate_trim)) + 
  facet_wrap_equal(~variable, scales="free") +
  geom_point(alpha=0.3) +
  geom_abline(slope = 1, intercept = 0, linetype=2, alpha=0.5) + 
  labs(
    x = "Untrimmed Estimate",
    y = paste0(q0*100, "% Trimmed Estimate")
  )


ggsave(file.path(image_path, "SI", "trimmed_v_untrimmed.png"), width = 8, height = 6)

#filter(stops0, grepl("229", location), grepl("PNC", variable), value != 500000) %>% summarize( mean(value))

```




## Design performances 






### Non-normalized Boxplots 

* vertical line is the model performance of the "full" campaign (n=278 sites x ~ 26 visits/site) 

```{r, fig.height=10}
full_campaign <- model_eval %>%
  filter(grepl("gs_", reference),
         grepl("full", design),
         ) %>%
  distinct(#mean_min_train_to_pred_dist_km, training_monitors_per_100km2, 
    out_of_sample, variable, reference, RMSE, MSE_based_R2#, reg_based_R2
    )


eval_cols <- c("RMSE", "MSE_based_R2"#, "reg_based_R2"
               )

```





non-normalized boxplots  
  * using gold standard observations as a reference 

```{r}
model_eval %>%
    filter(grepl("gs_", reference),
           !grepl("full|distance", design) #|fewer sites|fewer visits
            ) %>% 
    mutate(
      version = factor(str_to_title(version), levels = c(str_to_title(version_levels), "Business \n& Rush" )),
      design = factor(str_to_title(design), levels = c("Full", "Fewer Total Stops", "Balanced Seasons", "Fewer Days", "Fewer Hours")),
      
      total_stops2 = cut(total_stops, breaks = seq(0,8000,1000), labels = c(seq(1000,7000,1000), 7080)
                         )
      
    ) %>% 
  group_by(total_stops2) %>%
  summarize(
    min = min(total_stops),
    max = max(total_stops),
    unique = paste(sort(unique(total_stops)), collapse = ", ")
  ) %>%
  kable(caption = "Total number of stops in each bin") %>%
  kable_styling()

```



```{r}
for (i in seq_along(eval_cols[1:2])) {
  # i=2
  p <- model_eval %>%
    filter(grepl("gs_", reference),
           !grepl("full|distance", design)
            ) %>% 
    mutate(
      version = factor(str_to_title(version), levels = c(str_to_title(version_levels), "Business \n& Rush" )),
      design = factor(str_to_title(design), levels = c("Full", "Fewer Total Stops", "Balanced Seasons", "Fewer Days", "Fewer Hours")),
      
      #bin fewer total stops
      version = ifelse(grepl("total stops", design, ignore.case = T), 
                       as.character(cut(total_stops, breaks = seq(0,8000,1000), labels = c(seq(1000,7000,1000), 7080),  dig.lab=5,)), 
                       as.character(version))
    ) %>% 
    
    ggplot(aes(x = !!as.symbol(eval_cols[i]), y=version, col = out_of_sample)) + 
    facet_grid(design~variable, scales = "free", switch = "y", space = "free_y"
               ) + 
    geom_boxplot() +
    geom_vline(data=full_campaign, aes(xintercept=!!as.symbol(eval_cols[i]), col = out_of_sample)) +
    labs(col = "Out-of-Sample Set")
  
  
  print(p)
  
  ggsave(file.path(image_path, "SI", paste0("model_eval_", str_extract(eval_cols[i], "RMSE|R2"), ".png")),
         width = 9, height = 11)
  
}
 
```

### Spatial-Temporal Designs (sites x vistis)


```{r}
site_visit_summary0 <- normalized_model_eval %>%
  filter(
    grepl("CV", out_of_sample),
    grepl("fewer total stops", design),
    #don't use regression-based R2
    !grepl("reg_based_R2", model_eval),
    #grepl("gs_", reference)
    ) %>%
  # # separate version
  # separate(version, c("visits", "sites"), sep = " ", remove=F) %>%
  mutate(
    # visits = gsub("_.*", "", visits),
    # sites = gsub("_.*", "", sites),
    # #reorganize
    # visits = factor(visits, sort(as.numeric(unique(visits)) )),
    # sites = factor(sites, sort(as.numeric(unique(sites)) )),
    # total_stops = as.numeric(as.character(sites)) * as.numeric(as.character(visits)),
    model_eval = gsub("MSE_based_", "", paste("Normalized", model_eval)),
    )  

site_visit_summary <- site_visit_summary0 %>%
  group_by(design, visits, sites, total_stops, variable, model_eval, reference) %>%
  summarize(
    no_sims = n(),
    median_value = round(median(normalized_value), 2),
    iqr_value = IQR(normalized_value)
  )  

```

Mean R2, RMSE for visits x sites, by reference estimate used

```{r}

print("Mean normalized MSE-based R2 and RMSE by number of total stops in a campaign. Prediction performance parameters are calculated using either the full campaign or the simulated campaign estimates. Mean performance parameters are each based on 30 campaign simulations.")

#plot_vars <- c("gs_estimate", "campaign_estimate")
plot_vars <- c("R2", "RMSE")

for (i in plot_vars) {
  # i = "R2" # i = "RMSE"
  
  # reference lines
  if(grepl("R2", i)) {
    hline <- 0.85
  } else {
      hline <- 1.15
      }
  
  temp <- site_visit_summary %>%
    filter(#grepl(i, reference)
           grepl(i, model_eval)
           ) %>%
    mutate(sites = as.numeric(as.character(sites)),
           visits = factor(as.character(visits), levels = unique(site_visit_summary$visits)),
           reference = ifelse(grepl("gs_", reference), "Full Campaign Est", "Simulated Campaign Est"),
           reference = paste("Reference: ", reference)
           ) 
    
    p <- ggplot(data=temp, aes(y=median_value, x=total_stops, col=sites, 
               shape=visits
               )) +
      facet_grid(cols = vars(variable), rows=vars(reference), switch = "y", scales = "free",) +
      geom_hline(yintercept = hline, alpha=0.5, linetype=2) +
      #geom_line(stat="smooth", aes(group=reference), se=F, alpha=0.5, size=1, show.legend = F) +
      
      #geom_line(stat="smooth", aes(group=sites), se=F, alpha=0.65, size=0.5,) +
      
      geom_point(alpha=0.9) +
      # make higher site counts darker
      scale_color_continuous(high = "#132B43", low = "#56B1F7") +
      
      scale_shape_manual(values=1:length(unique(site_visit_summary$visits))) +
      labs(x = "Total Stops", y = paste("Median Normalized", i), col = "Sites", shape = "Visits per Site")
    
    print(p)
  
  # save images. RMSE will go in the SI
  if(grepl("RMSE", i)) {
    p_image_path = file.path(image_path, "SI")
    } else{
      p_image_path = image_path
      }
  
  ggsave(file.path(p_image_path, paste0("total_stops_performance_",i, ".png")), width = 7, height = 8)
  
  # #save exmple with just PNC using the gold standared reference
  # p %+% filter(temp,
  #               grepl("Full", reference),
  #               grepl("PNC", variable)
  #               ) %>%
  #   print()
  # 
  # ggsave(file.path(image_path, "SI", paste0("total_stops_performance_pnc_",i, ".png")), width = 7, height = 8)
  
}

```


same as above, but plotting the IQR (vs mean) of R2, RMSE across 30 campaigs  


```{r}

p_image_path = file.path(image_path, "SI")

lapply(plot_vars, function (x) {
  p <- site_visit_summary %>%
    filter(grepl(x, model_eval)) %>%
    mutate(sites = as.numeric(as.character(sites)), 
           visits = factor(as.character(visits), levels = unique(site_visit_summary$visits)),
           reference = ifelse(grepl("gs_", reference), "Full Campaign Est", "Simulated Campaign Est"),
           reference = paste("Reference: ", reference)
           ) %>%
    
    ggplot(aes(y=iqr_value, x=total_stops, col=sites, shape=visits)) +
    facet_grid(cols = vars(variable), rows=vars(reference), switch = "y") +
    #geom_line(stat="smooth", aes(group=sites), se=F, alpha=0.65, size=0.5) +
    geom_point(alpha=0.9) + 
    scale_color_continuous(high = "#132B43", low = "#56B1F7") +
    scale_shape_manual(values=1:length(unique(site_visit_summary$visits))) +
    labs(x = "Total Stops", y = paste("IQR of Normalized", x), col = "Sites", shape = "Visits per Site")
  }) %>%
  ggarrange(plotlist = ., common.legend = T, legend = "bottom")

ggsave(file.path(p_image_path, paste0("total_stops_performance_sd.png")), width = 15, height = 8)

```


sites x visit ranges needed to reach normalized R2=0.85

```{r}
decent_value <- 0.85
buffer_value <- 0.03
low_value <- decent_value - buffer_value
high_value <- decent_value + buffer_value

site_visit_summary %>%
  filter(
    grepl("total stops", design),
    grepl("gs_", reference),
    model_eval == "Normalized R2",
     median_value >= low_value & median_value <= high_value
    ) %>%
  group_by(variable) %>% #View()
  summarize(
    total_stops = paste(range(total_stops), collapse = "-"),
    sites = paste(range(sites), collapse = "-"),
    visits = paste(range(visits), collapse = "-"),
  ) %>%
  kable(caption = paste0("Total campaign stops associated with median normalized R2 values between ", low_value, "-", high_value ),
        col.names =  c("Pollutant", "Total Stops", "Sites", "Visits per Site")
          ) %>%
  kable_styling()


```




```{r, R2 table, eval=F}

# R2
decent_value_start <- 0.85
decent_value_end <- decent_value_start + 0.04

site_visit_summary %>%
  filter(grepl("R2", model_eval),
         median_value >= decent_value_start & median_value < decent_value_end) %>% 
  mutate_at(vars(visits, sites), ~as.numeric(as.character(.))) %>%
  group_by(variable) %>%  
  summarize(
    #min = min(total_stops),
    mean_range_total_stops = paste0(round(mean(total_stops)), " (", min(total_stops), "-", max(total_stops), ")"),
    #max = max(total_stops)
    mean_range_sites = paste0(round(mean(sites)), " (", min(sites), "-", max(sites), ")"),
    mean_range_visits = paste0(round(mean(visits)), " (", min(visits), "-", max(visits),  ")"),
    ) %>%
  kable(caption = paste0("Number (and range) of stops (sites x visits) that produce normalized MSE-based R2 values >= ", decent_value_start, " and < ", decent_value_end),
        col.names = c("Pollutant", "Total Stops", "Sites", "Visits")
        ) %>%
  kable_styling()


```

```{r, RMSE table, eval=F}
# RMSE 
decent_value_start <- 1.15
decent_value_end <- decent_value_start + 0.03

site_visit_summary %>%
  filter(model_eval == "RMSE",
         median_value >= decent_value_start & median_value < decent_value_end) %>% #View() 
  mutate_at(vars(visits, sites), ~as.numeric(as.character(.))) %>%
  group_by(variable) %>%
  summarize(
    #min = min(total_stops),
    mean_range_total_stops = paste0(round(mean(total_stops)), " (", min(total_stops), "-", max(total_stops), ")"),
    #max = max(total_stops)
    mean_range_sites = paste0(round(mean(sites)), " (", min(sites), "-", max(sites), ")"),
    mean_range_visits = paste0(round(mean(visits)), " (", min(visits), "-", max(visits),  ")"),
    
    #sites_range = paste(min(sites), max(sites), sep = "-"),
    #visit_range = paste(min(visits), max(visits), sep = "-"),
    
  ) %>%
  kable(caption = paste0("Number (and range) of stops (sites x visits) that produce normalized RMSE values >= ", decent_value_start, " and < ", decent_value_end),
        col.names = c("Pollutant", "Total Stops", "Sites", "Visits")
        ) %>%
  kable_styling()
  
    
```




 


```{r, very old v1 plots, eval=F}

# very old plots 
print("lines and points are for total stops")

for(i in c("MSE_based_R2", "RMSE") ) {
  # i = "RMSE"
  # i = "R2"

  df <- filter(site_visit_summary, model_eval == i)
  
  p <- df %>%
    ggplot(aes(y=visits, x=sites)) +
    facet_grid(rows = vars(variable), scales = "free", space = "free", switch = "y") +
    geom_bin2d(aes(fill=median_value),) +
    #reverse colors
    #scale_fill_binned(low="#56B1F7", high="#132B43", breaks = seq(0.1,1,0.1)) +
    
    ## reverse blues
    #scale_fill_continuous(trans = 'reverse') + 
    
    scale_colour_viridis_c(direction=-1) + #scale_color_binned( type = "viridis") +
    #scale_colour_binned(low="white", high="black", breaks = seq(1e3,7e3,2e3)) +
    
    geom_line(aes(group=total_stops, col=total_stops), size=1.5, alpha=0.8) +
    geom_point(aes(group=total_stops, col=total_stops,)) +
    
    
    
    theme(legend.position = "right") +
    labs(fill = paste0("Normalized\n", i),
         col = "Total Stops"
         )
  p
  
  #reverse blues
  #if(i == "RMSE") { p <- p +  scale_fill_continuous(trans = 'reverse')} 
  
  print(p)
  
  ggsave(file.path(image_path, paste0("visit_site_hist_", str_extract(i, "RMSE|R2"), ".png")), 
         plot=p, 
         width = 7, height = 9
         )
  
}

```


```{r, old v2 plots, eval=F, fig.height=10}
# * 2-D histograms 
# 
# * plots are for randomly selected sites and visits, so these simulations are for campaigns that are balanced, year-around (the full campaign)
# 
# * **note that lines that appear to be overlapping do not share points. e.g., 100 sites x 24 visits is only connected to 250 sites x 8 visits, and no other points in between.** 
# 
# * the ability for total sites to affect model performance improves all pollutant models



for(i in c("MSE_based_R2", "RMSE") ) {
  # i = "RMSE"
  # i = "MSE_based_R2"

  df <- filter(site_visit_summary, model_eval == i)
  
  p <- df %>%
    ggplot(aes(y=visits, x=sites)) +
    facet_grid(rows = vars(variable), scales = "free", space = "free", switch = "y") +
    geom_bin2d(aes(fill=total_stops)) +
  
    #label R2/RMSE
    stat_bin2d(geom="text", aes(label=median_value, col=median_value)) +

    viridis::scale_fill_viridis() +
    viridis::scale_colour_viridis(option="C" , direction = -1 ) +
    
    theme(legend.position = "right") +
    labs(fill = "Total Stops",
         col = paste0("Normalized\n", i)
         )
  print(p)

  ggsave(file.path(image_path, paste0("visit_site_hist2_", str_extract(i, "RMSE|R2"), ".png")), plot=p, width = 7, height = 9)
  
}



```




* linear models

$$R^2_{MSE-based} = \alpha + \beta_1total\ sites + \epsilon$$


$$\hat{R^2}_{MSE-based} = \hat{\alpha} + \hat{\beta_1}*total\ sites$$

  * estimates are based on the number of simulations for each visit-site combination
  
 


```{r, eval=F}
### --> can you interpret this output?


# x = group_split(site_visit_summary0, variable, eval_summary)[[2]] #  R2

site_visit_lms <- lapply(group_split(site_visit_summary0, variable, eval_summary), FUN = function(x) {
  lm1 <- x %>%
    mutate(
      total_stops = total_stops/1000,
      
      sites = as.numeric(as.character(sites)),
      visits = as.numeric(as.character(visits)),
    ) %>% 
    
    lm(value ~ total_stops#*stop_kind
    #lm(value ~ sites + visits
       , data = .) %>%
    tidy() %>%
    mutate(
      variable = first(x$variable),
      eval_summary = first(x$eval_summary)
    )
  
  lm1
  
  }
  ) %>%
  bind_rows()


site_visit_lms %>%
  filter(
    !grepl("reg_based", eval_summary)
  ) %>%
  #arrange(eval_summary, desc(term), ) %>%
  kable(caption = "linear regression models for RMSE and R2 estimates based on the total number of stops (sites x visits/site). Total stops are in 1,000s") %>%
  kable_styling()

```

 

```{r}
# - plot lm estimates
# * for all pollutants, adding one additional visit per site improves the UK-PLS model peformance to a larger degree than adding one additional site with the same number of visits
#     * does this have to do with the total number of stops?
#     
#     e.g.,
#     25 sites x 4 visits/site = 100 stops.      
#       1 additional site = 4 extra stops    
#       1 additional visit/site = 35 extra stops 
#     
#     
# 
# * for PNC, BC and NO2, increasing the number of sites rather than the number of visits per site improves R2 to a larger degree than it does for PM2.5 and CO2. Is it because These pollutants have more spatial variability.

```

```{r, eval=F}
#R2
site_visit_lms %>%
  filter(eval_summary == "MSE_based_R2") %>%
  {
  ggplot(., aes(fill=term, y=estimate,
             x = variable
             )) + 
  #facet_wrap(variable~eval_summary, scales="free", ncol = 2 ) + 
  geom_bar(stat = "identity", position = "dodge") + 
  labs(
    y = paste0("LM estimate for ", first(.$eval_summary), ""),
    title = "linear regression models for R2 based on the total number of stops (sites x visits/site).\nTotal stops are in 1,000s"
  )
  }


```



### Temporal Designs

lines connecting the median (IQR) normalized value vs GS estimates only
  * using random CV predictions   
  * the denominator (normalizer) is the full campaign performance   
* both references: GS and campaign estimates

```{r}
# 12 visit mean performance ref
temp <- site_visit_summary %>%
  ungroup() %>%
  filter(sites=="278", visits == "12", reference == "gs_estimate") %>%
  rename(normalized_value = median_value) %>%
  select(-c(design))

```

```{r}
p <- normalized_model_eval %>%
  filter(!grepl("reg_based_R2", model_eval),
         #grepl("gs_", reference),
         !grepl("distance|fewer total stops", design),
         !grepl("full", design),
         grepl("CV", out_of_sample)
         ) %>%  
  # #calculate normalized RMSE, using ___ value as denominator  
  mutate(model_eval = ifelse(grepl("RMSE", model_eval), "Normalized RMSE", "Normalized R2" ),
         version = ifelse(version == "business & rush", "business \n& rush", as.character(version)),
         version = factor(version, levels = c(version_levels, "business \n& rush" )),
          version = str_to_title(version),
         design = str_to_title(design),
         reference = ifelse(grepl("gs_", reference), "Full\nCampaign\nEst", "Simulated\nCampaign\nEst")
         ) %>% 

  ggplot(aes(y = normalized_value, x=version, col = variable, fill=variable, #group=variable,
             group= interaction( variable, reference),
             linetype = reference
             )) + 
  facet_grid(cols = vars(design), rows = vars(model_eval), scales = "free", switch = "y") +
  geom_smooth(stat = 'summary', fun.data = median_hilow, fun.args = list(conf.int = 0.5), alpha = 0.2) + 
  
  geom_hline(data=temp, aes(yintercept=normalized_value, col=variable), linetype=3, alpha=1) +

  labs(col = "Pollutant", fill = "Pollutant", x= "", y = "", linetype = "Reference")

print(p)

ggsave(file.path(image_path, paste0("campaign_estimates_model_eval2_.png")), width = 10, height = 6)

```


same as above, but only using gold standard reference estimates & showing a 95% CI

### --> make eval=T. ERROR:  spatial_temporal not found 

```{r, eval=F}

temp <- normalized_model_eval %>%
  filter(!grepl("reg_based_R2", model_eval),
         grepl("gs_", reference),
         grepl("CV", out_of_sample),
         grepl("balanced seasons|fewer days|fewer hours", design) | (grepl("total stops", design) & visits == "12" & sites == "278")
         ) %>% 
  # #calculate normalized RMSE, using ___ value as denominator  
  mutate(model_eval = ifelse(grepl("RMSE", model_eval), "Normalized RMSE", "Normalized R2" ),
         version = ifelse(version == "business & rush", "business \n& rush", as.character(version)),
         version = factor(version, levels = c(version_levels, "business \n& rush" )),
          version = str_to_title(version),
         design = str_to_title(design),
         reference = ifelse(grepl("gs_", reference), "Full\nCampaign\nEst", "Simulated\nCampaign\nEst")
         ) 


lapply(c("temporal", "spatial temporal"), function (x) {
  
  #plot in main text but with a wider range
  if(x=="temporal") {
    # x = "temporal"
    
    p <- filter(temp, spatial_temporal==x) %>%
      ggplot(aes(y = normalized_value, x=version, col = variable, fill=variable,
                 group= interaction( variable, reference), linetype = reference)) +
      facet_grid(cols = vars(design), rows = vars(model_eval), scales = "free", switch = "y") +
      labs(col = "Pollutant", fill = "Pollutant", x= "", y = "", linetype = "Reference") +
      geom_smooth(stat = 'summary', fun.data = median_hilow, fun.args = list(conf.int = 0.95), alpha = 0.2)
    }
  
  # refernce plots 
  if(x=="spatial temporal") {
    # x="spatial temporal"
    df <- filter(temp, spatial_temporal==x) %>% 
      group_by(variable, model_eval, reference, design) %>%
      summarize(
        ymin = quantile(normalized_value, 0.05),
        median = median(normalized_value),
        ymax = quantile(normalized_value, 0.95),
        ) %>%
      ungroup() 
    
      p <- lapply(c("Normalized R2", "Normalized RMSE"), function (b) {
        
        if(b == "Normalized R2") {
          #b = "Normalized R2"
          
          filter(df, model_eval == b) %>%
          ggplot() +
            facet_grid(cols = vars(design), rows = vars(model_eval), scales = "free", switch = "y") +
      geom_errorbar(aes(y=median, ymin = ymin, ymax = ymax, x=variable, col=variable)) + 
            scale_y_continuous(limits = c(0,1))
        }
        
        if(b == "RMSE") {
          # b = "Normalized RMSE"
          
          filter(df, model_eval == b) %>%
          ggplot() +
            facet_grid(cols = vars(design), rows = vars(model_eval), scales = "free", switch = "y") +
      geom_errorbar(aes(y=median, ymin = ymin, ymax = ymax, x=variable, col=variable)) + 
            scale_y_continuous(limits = c(1,4))
          
        }
        
        }) %>%
        ggarrange(plotlist = .)
  }
  
  #print(p)
  
      }) %>%
  ggarrange(plotlist = ., common.legend = T, legend = "bottom", align = "h")




```

 

 

```{r, gs_ref only, eval=F}
# same as above but only w/ comparisons vs the GS esttimates
#* the denominator for all other designs is the median, random 12 visit performance since these designs collect 12 visits

normalized_model_eval %>%
  #gather("model_eval", "value", c("RMSE", contains("_R2"))) %>%
  filter(!grepl("reg_based_R2", model_eval),
         grepl("gs_", reference),
         !grepl("distance|fewer total stops", design),
         !grepl("full", design),
         grepl("CV", out_of_sample)
         ) %>%  
  # #calculate normalized RMSE, using ___ value as denominator  
  # group_by(variable, model_eval, out_of_sample) %>%  
  mutate(model_eval = ifelse(grepl("RMSE", model_eval), "Normalized RMSE", "Normalized MSE-based R2" ),
         version = ifelse(version == "business & rush", "business \n& rush", as.character(version)),
         version = factor(version, levels = c(version_levels, "business \n& rush" )) 
         ) %>% 

  ggplot(aes(y = normalized_value, x=version, col = variable, fill=variable, #group=variable,
             group= interaction( variable, out_of_sample),
             #linetype = out_of_sample
             )) + 
  facet_grid(cols = vars(design), rows = vars(model_eval), scales = "free", switch = "both") +
  geom_smooth(stat = 'summary', fun.data = median_hilow, fun.args = list(conf.int = 0.5), alpha = 0.2) + 
  
  geom_vline(xintercept = "12", linetype=3, alpha=0.25) +
  geom_hline(yintercept = 1, linetype=2, alpha=0.5) +
  labs(col = "Pollutant", fill = "Pollutant", x= "", y = "" )


ggsave(file.path(image_path, "normalized_model_eval.png"), width = 8, height = 6
       #width = 8, height = 10
       ) 

```


 




 


```{r, eval=F}
### for presentations, focus on BC



# plot for spatial & temporal sims separately
#st <- setdiff(unique(model_eval$spatial_temporal), "spatial temporal")

var1 <- "BC"

pacman::p_load(ggpmisc)

#for(i in seq_along(st)) {
  # i=1
  #p <- 
    model_eval %>%
    filter(grepl("gs_", reference),
             #don't include st ref or distance analyses
             !grepl("full|distance", design),
             grepl(var1, variable),
             #spatial_temporal == st[i]
             ) %>%  
    #gather("summary", "value", RMSE, MSE_based_R2) %>% #View()
      
    ggplot(aes(x = MSE_based_R2, y=version, col = out_of_sample)) +
    #ggplot(aes(x = value, y=version, col = out_of_sample)) + 
      facet_grid(design~variable, scales = "free", switch = "y", 
                 space = "free" # +summary
                 ) + 
      geom_boxplot() +
      geom_vline(data=filter(full_campaign, grepl(var1, variable) ), aes(xintercept=MSE_based_R2, col = out_of_sample)) +
    annotate(geom="table", 
             x=-Inf, y=Inf, 
             label = list("test")
             ) +
      
      labs(col = "Validation Set")
  
  #print(p)
  
  ggsave(file.path(image_path, "Other", paste0(var1, "_model_eval_R2.png") ), width = 6, height = 8)


#}
```

```{r, eval=F}
### Monitor Density - fewer sites 


spatial_designs <- model_eval %>%
  filter(
    #grepl("spatial", spatial_temporal, ignore.case = T) & !grepl("full", design),
    grepl("fewer sites", design), 
    grepl("gs_estimate", reference),
    )  
      
        
```

```{r, eval=F}
# crosswalk table 
 
#spatial_designs %>%
model_eval %>%
  filter(
    grepl("fewer sites$|full", design),  
    grepl("gs_estimate", reference),
    !grepl("Routes", out_of_sample)
    ) %>%
  group_by(design, 
           version, out_of_sample) %>%
  summarize(
   prediction_sites = unique(prediction_sites),
   
   mean_min_train_to_pred_dist_km = mean(mean_min_train_to_pred_dist_km),
   training_monitors_per_100km2 = unique(training_monitors_per_100km2)
   ) %>%
  kable(caption = "Spatial characteristics of fewer sites simulations",
        col.names = c("Design", "Version (No. Training Sites)", "Validation Set", "Validation (Prediction) Sites", "Mean Training to Prediction Site Dist (km)", "Training Monitors per 100 km2"
                      ), 
        digits=2
        ) %>%
  kable_styling() %>%
  add_footnote(
    c("distances change across simulations; showing the overall mean.")
  )
  

```

```{r, eval=F}
#100 km2 area relative to the monitoring area  
 
# make a 100 km2 circle

#ggplot(data=loc_lat_long, aes(label=location)) + geom_sf_label()

monitoring_area <- round(set_units(st_area(monitoring_region), "km2"))


#find radius needed to make a 100 km2 circle
# A (km2) = pi*r2
r <- sqrt(100/pi)

loc1 <- "MC0002"

pt <- loc_lat_long %>%
  filter(location == loc1
         ) %>%
  st_transform(m_crs) %>%
  #convert km radius to m radius
  st_buffer(r*10^3)

# #check that things work. # looks good   
# st_area(pt) %>% units::set_units("km2")

# plot
ggplot(data=monitoring_region) + 
  geom_sf(aes(fill = "monitoring region")) + 
  geom_sf(data=pt, aes(fill = "100 km2 area")) + 
  geom_sf(data=filter(loc_lat_long, location == loc1), #aes(label=location)
          ) + 
  labs(fill = "",
       title = "Example of a \n100 km2 area"
       ) + 
  theme(plot.title = element_text(size=22))

ggsave(file.path(image_path, "SI", "100km2_area.png"), width = 5, height = 9)

```



 



## Prediction Distance

example of geographic distance buffered LOO 

```{r}

r <- 2000
loc1 <- "MC0002"
buf <- loc_lat_long %>%
  filter(location == loc1) %>%
  st_transform(m_crs) %>%
  #convert km radius to m radius
  st_buffer(r)

# plot
ggplot(data=monitoring_region) + 
  geom_sf() +
  geom_sf(data=filter(loc_lat_long, set == "Simulation Set"), aes(col = "Training Sites")) + 
  geom_sf(data=buf, aes(fill = "2000 m buffer")) + 
          scale_fill_grey(start=0.8) + #scale_fill_manual(values=c("#999999")) +
  geom_sf(data=filter(loc_lat_long, location == loc1), aes(col = "Prediction Site")) + 
  labs(fill = "",
       col = "Buffered LOO"
       ) 
   
ggsave(file.path(image_path, "SI", "geo_dist_BLOO_exp.png" ), width = 5, height = 6)

```





average nearest training monitor distance 

```{r}
# fn returns summary statistics for the distance between locations (default = min, or closest monitor)

pt_dist <- function(locs1, locs2, summary_stat = "min", 
                    loc_lat_long. = loc_lat_long
                    ) {
  #make datasets into sf objects
  locs1 <- filter(loc_lat_long., location %in% locs1)
  locs2 <- filter(loc_lat_long., location %in% locs2)
  

  dist_matrix <- st_distance(locs1,locs2) %>%  
    #make numeric
    drop_units() %>%
    #replace all "0" distances (for same sites - e.g. training-training pt distances) w/ NA
    na_if(0)
  
  #calc distance summary statistic (e.g. mean/min/max) of each dataset
  result <- apply(dist_matrix, 1, summary_stat, na.rm=T) %>% 
    summary()
    #overall avg for both datasets
    #mean()
  
  return(result)
  
}

```

distribution of closest training monitor (n=308 minimum distances)

```{r}

pt_dist(loc_lat_long$location,loc_lat_long$location)

```





* geographic distance

non-normalized values
 
### --> update using new cluster/pca code
 
```{r}

#for (i in eval_cols[1:2]) {
  # i=eval_cols[2]

lapply(eval_cols[c(1,2)], function(i) {
    
  p <- model_eval %>%
      filter(
      grepl("distance", design), 
      grepl("gs_estimate", reference),
      ) %>%  
      mutate(version = as.numeric(as.character(version)),
             design = ifelse(grepl("PCA", design), "PCA Covariate Distance", "Geographic Distance (m)")
             ) %>%
      
      ggplot(aes(x = version, y= !!as.symbol(i), col=variable)) +
      #facet_grid(rows=vars(variable), cols=vars(design), switch = "y", scales = "free_x") +  
      geom_point() + 
      geom_line() +
       # geom_hline(data=full_campaign, linetype=2, alpha=0.5, aes(yintercept = !!as.symbol(i), col=out_of_sample)) +
      labs(x = "Nearest Training Site",
           col = "Pollutant"
           ) 
    
    if(grepl("R2", i)) {
      p <- p +
        #things can go on the same scale
        facet_grid(rows= vars("All Pollutants"), cols=vars(design), switch = "y", scales = "free_x") + 
        labs(y= "MSE-based R2")
      }
    
    if(grepl("RMSE", i)) {
      p <- p + 
        # each pollutant needs its own scale for RMSE
        facet_grid(rows=vars(variable), cols=vars(design), switch = "y", scales = "free") 
      }
  
  print(p)
    
    }) %>%
  ggarrange(plotlist = ., common.legend = T, legend = "bottom",
            nrow = 2, align = "v", 
            heights = c(5, 1.7)
            )

ggsave(file.path(image_path, "SI", "non_normalized_distance.png"), width = 6, height = 8)

```

same as above, but scaled relative to non-buffered LOO CV

```{r}
lines1 <- data.frame(model_eval = c("Normalized RMSE", "Normalized R2"), #eval_cols[1:2],
                     value = c(1.15, 0.85)
                     )

normalized_model_eval %>%
    filter(
      !grepl("reg_", model_eval),
      grepl("distance", design), 
      grepl("gs_estimate", reference),
      ) %>%  
      mutate(version = as.numeric(as.character(version)),
             model_eval = ifelse(model_eval == "RMSE", "Normalized RMSE", "Normalized R2"),
             design = ifelse(grepl("PCA", design), "PCA Covariate Distance", "Geographic Distance (m)")
             ) %>%
    
      ggplot(aes(x = version, y= normalized_value, col = variable)) +
      facet_grid(rows=vars(model_eval), 
                 cols=vars(design), scales = "free", switch = "y") +  
      geom_point() + 
      geom_line() +
      
      geom_hline(data=lines1, aes(yintercept = value), 
                 linetype=2, alpha=0.5
                 ) +  
    
      labs(col = "Polluant",
           x = "Buffer (Nearest Training Site)", 
           y = ""
           ) 
  
ggsave(file.path(image_path, paste0("distance_relative.png")), width = 8, height = 7)
    
```

### training sites in each bufferred LOO

```{r}
sites_used_geo_dist  %>%
  mutate_if(is.numeric, ~round(., 1)) %>%
  mutate(
        min = paste0(min, " (", min_pt, "%)"),
        median = paste0(median, " (", median_pt, "%)"),
        max = paste0(max, " (", max_pt, "%)"),
      ) %>%
      select(-contains("_pt" ), -n) %>%
  
  kable(caption = "Distribution of the number of sites included in each buffered leave-one-out validation model using geographic distance (N = 278 total sites)",
        format.args = list(big.mark=","), col.names = c("Buffer (m)", "Min", "Median", "Max") ) %>%
  kable_styling()




sites_used_pca_dist  %>%
  mutate_at(vars(contains("_pt")), ~round(., 1)) %>% # View()
  mutate(
        min = paste0(min, " (", min_pt, "%)"),
        median = paste0(median, " (", median_pt, "%)"),
        max = paste0(max, " (", max_pt, "%)"),
      ) %>%
      select(-contains("_pt" ), -n) %>%
  
  kable(caption = "Distribution of the number of sites included in each buffered leave-one-out validation model using geographic distance (N = 278 total sites)",
        format.args = list(big.mark=","), col.names = c("Buffer (Score)", "Min", "Median", "Max") ) %>%
  kable_styling()


```



```{r, eval=F}

# # Variogram Parameters
# 
# * note that fitted models did not necessarily use Exponential variogram fits 

# see range parameter
lapply(modeling_parameters, function(x) {
  x$variogram_model %>%
    mutate(variable = x$variable)
  }) %>%
  bind_rows() %>%
  filter(model != "Nug") %>%
  select(variable, range) %>%
  arrange(range) %>%
  mutate(range = round(range)) %>%
  label_pollutant() %>%
  kable(caption = "Range parameters for exponential variograms fit to the residuals of pollutant LUR models", 
        col.names = c("Pollutant", "Range (m)"), format.args = list(big.mark=",")
        ) %>%
  kable_styling()
  




```



# Maps 


Disc: “You can produce very different results at prediction locations of interest with restricted sampling designs. Others have reported similar findings.(Hatzopoulou et al., 2017; Saha et al., 2019)  

```{r}
   
# relabel designs & select a subset of the designs to plot 
df0 <- grid_predictions %>%
  # calculate the median prediction error for any givens site
  
  
mutate(
      design = ifelse(design == "Full", "Full Campaign",
                       ifelse(design == "Fewer Total Stops", paste0(version, " Stops"),
                              ifelse(design == "Balanced Seasons", paste0(version, " Season"),
                                     ifelse(design == "Fewer Days", as.character(version), 
                                            ifelse(design == "Fewer Hours", paste0(version, " Hours"), NA
                                                   ))))),
      design = factor(design, levels = c("1 Season",  "2 Season",  "Weekday", "Rush Hours", "Business Hours","1000 Stops", "3000 Stops",  "Full Campaign"))) %>%
    
    #for now..
    filter(!design %in% c("1 Season", "Weekday", "1000 Stops"))


```


```{r}
# fn returns maps of the grid predictions. Each pollutant has its own legend.

# dt = df0 %>%
#   filter(grepl("Full", design),
#          grepl("PNC", variable),
#          ) 
# var = "prediction_median"
# name="Conc." 
# x= "PNC (pt/cm3)"

map_predictions <- function(dt, var = "prediction_median", map_type="", ...) {
  
  dt <- rename(dt, value = all_of(var))
  
  prediction_maps <- lapply(var_names, function(x) {
    
    p <- filter(dt, variable == x) %>%
      ggplot() +
      geom_sf(aes(col=value), alpha=0.8, size=1) +
      scale_colour_gradient(low = "yellow", high = "red", ...) +
      facet_grid(cols = vars(variable), rows = vars(design), switch = "y")  +
      theme_void() +
      theme(legend.justification=c(0,1), legend.position = c(1,1),
            #legend.title = element_blank()
            )
    
    #remove labels from 2+ columns
    if(!grepl("PNC", x)) {
      p <- p + theme(strip.background.y = element_blank(), strip.text.y = element_blank())
      }
    
    if(map_type=="prediction_difference") {
      p <- p + scale_colour_gradient2(low = "#56B1F7", high = "red", ...)
      }
    
    print(p)
    
    })
  
  prediction_maps %>% 
    ggarrange(plotlist = ., ncol = length(var_names)) 

}

```
 

## SI 

```{r}

# Full campaign site predictions
full_map <- df0 %>%
  filter(grepl("Full", design)) %>%
  map_predictions(., var = "prediction_median", name="Conc.")

# Median site prediction difference relative to the full campaign 
difference_map <- df0 %>%
  filter(!grepl("Full", design)) %>%
  map_predictions(., var = "median_prediction_diff", map_type = "prediction_difference", name="Difference")  

print("Comparison of the full campaign exposure surface (top) and the median prediction difference for some example monitoring designs.")

ggarrange(full_map, difference_map, ncol = 1, heights = c(1,4))

ggsave(file.path(image_path, "SI", paste0("maps_of_prediction_diff.png")),  height = 12, width = 12)



# map the IQR of the predictions
print("IQR of site predictions following different sampling designs. Larger values indicate larger degrees of variability for any given location across all simulated campaigns.")

iqr_map <- df0 %>%
  # the full design has 1 sim and so IQR=0
  filter(!grepl("Full", design)) %>%
  map_predictions(., var = "prediction_iqr", name="IQR")

ggarrange(full_map, iqr_map, ncol = 1, heights = c(1,4))

ggsave(file.path(image_path, "SI", paste0("maps_of_prediction_iqr.png")),  height = 12, width = 12)

```

## abstract art 

```{r}
conc_factor <- 1000
text_size <- 5

pnc_full_map <- df0 %>%
  filter(grepl("Full", design),
         grepl("PNC", variable),
         ) %>%
  mutate(
    prediction_median = prediction_median/conc_factor,
    design = "PNC\n7,300 Year-Around\nStops"
    ) %>%
   ggplot() +
      geom_sf(aes(col=prediction_median), alpha=0.8, size=1) +
      scale_colour_gradient(low = "yellow", high = "red", name="Prediction\n(10k pt/cm3)") +
      facet_wrap(~design, #strip.position = "bottom"
                 )  +
  theme_void() + 
  theme(
    legend.title = element_text( size=text_size), 
    legend.text=element_text(size=text_size),
    strip.text.x = element_text(size = text_size)
    )

#pnc_full_map 


pnc_sample_maps <- df0 %>%
  filter(!grepl("Full", design),
         grepl("PNC", variable),
         ) %>%
  mutate(
    design = gsub("Hours", "Hrs", design),
    design = gsub(" ", "\n", design),
    design = gsub("Season", "Seasons", design),
    design = gsub("000", ",000", design),
    design = relevel(factor(design), ref = "3,000\nStops"),
    
    median_prediction_diff = median_prediction_diff/conc_factor
  ) %>%
  ggplot() +
  geom_sf(aes(col=median_prediction_diff), alpha=0.8, size=1) +
  scale_colour_gradient2(low = "#56B1F7", high = "red", name="Difference")+
      facet_wrap(~design, #strip.position = "bottom"
                 )  +
      theme_void() + 
  theme(
    legend.title = element_text( size=text_size), 
    legend.text=element_text(size=text_size),
    strip.text.x = element_text(size = text_size)
    )

#pnc_sample_maps

ggarrange(pnc_full_map, pnc_sample_maps, widths = c(0.5,1))

ggsave(file.path(image_path, "toc_graphic.png"), width = 3.25, height = 1.75)

```

 

 




 